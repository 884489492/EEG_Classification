{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11168621,"sourceType":"datasetVersion","datasetId":6969728},{"sourceId":11210970,"sourceType":"datasetVersion","datasetId":7000383}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport pickle\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\nfrom scipy import signal\nfrom scipy.fft import fft\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntorch.manual_seed(42)\nnp.random.seed(42)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n\ndata_path = '/kaggle/input/deap123'\nall_data = []\nlabels = []\nfor file in os.listdir(data_path):\n    if file.endswith('.dat'):\n        with open(os.path.join(data_path, file), 'rb') as f:\n            subject_data = pickle.load(f, encoding='latin1')\n            all_data.append(subject_data['data'][:, :32, :])\n            labels.append(subject_data['labels'])\ndata = np.concatenate(all_data, axis=0)\nlabels = np.concatenate(labels, axis=0)\nvalence = labels[:, 0]\narousal = labels[:, 1]\nvalence_labels = (valence > 5).astype(np.int32)\narousal_labels = (arousal > 5).astype(np.int32)\nfs = 128\nfreq_bands = [(4, 8), (8, 13), (13, 30), (30, 45)]\n\n# 计算多种特征\ndef differential_entropy(data, axis=-1):\n    std = np.std(data, axis=axis, keepdims=True) + 1e-8\n    de = 0.5 * np.log(2 * np.pi * np.e * std**2)\n    return de.squeeze()\n\nde_data = []\npsd_data = []\ntime_data = []\nfor low, high in freq_bands:\n    b, a = signal.butter(4, [low, high], btype='band', fs=fs)\n    filtered = signal.filtfilt(b, a, data, axis=2)\n    de = differential_entropy(filtered, axis=2)\n    fft_data = np.abs(fft(filtered, axis=2))[:, :, :64]\n    psd = np.mean(fft_data ** 2, axis=-1)\n    time_mean = np.mean(filtered, axis=2)\n    de_data.append(de)\n    psd_data.append(psd)\n    time_data.append(time_mean)\nde_data = np.stack(de_data, axis=1)\npsd_data = np.stack(psd_data, axis=1)\ntime_data = np.stack(time_data, axis=1)\n\n# 分段为3秒片段\nsegment_len = fs * 3\nsegments = []\nde_segments = []\npsd_segments = []\ntime_segments = []\nfor i in range(de_data.shape[0]):\n    trial = data[i]\n    de_trial = de_data[i]\n    psd_trial = psd_data[i]\n    time_trial = time_data[i]\n    for j in range(0, trial.shape[-1] - segment_len + 1, segment_len):\n        segments.append(trial[:, j:j + segment_len])\n        de_segments.append(de_trial)\n        psd_segments.append(psd_trial)\n        time_segments.append(time_trial)\nsegments = np.array(segments)\nde_segments = np.array(de_segments)\npsd_segments = np.array(psd_segments)\ntime_segments = np.array(time_segments)\nvalence_labels = np.repeat(valence_labels, 8064 // segment_len)\narousal_labels = np.repeat(arousal_labels, 8064 // segment_len)\n\n# 标准化\nde_segments = (de_segments - de_segments.mean()) / (de_segments.std() + 1e-8)\npsd_segments = (psd_segments - psd_segments.mean()) / (psd_segments.std() + 1e-8)\ntime_segments = (time_segments - time_segments.mean()) / (time_segments.std() + 1e-8)\n\n# 脑区划分\nregions = {\n    'Frontal': [0, 1, 2, 3, 16, 17, 18, 19],\n    'Prefrontal': [4, 5, 20, 21],\n    'Parietal': [6, 7, 8, 9, 22, 23],\n    'Temporal': [10, 11, 12, 13, 24, 25],\n    'Occipital': [14, 15, 26, 27, 28, 29]\n}\nnum_nodes = len(regions)  # 5\n\n# 提取区域特征\ndef extract_region_features(de_data, psd_data, time_data, regions):\n    region_de = []\n    region_psd = []\n    region_time = []\n    for region in regions.values():\n        region_de.append(de_data[:, :, region].mean(axis=-1))\n        region_psd.append(psd_data[:, :, region].mean(axis=-1))\n        region_time.append(time_data[:, :, region].mean(axis=-1))\n    return np.stack(region_de, axis=2), np.stack(region_psd, axis=2), np.stack(region_time, axis=2)\n\nregion_de, region_psd, region_time = extract_region_features(de_segments, psd_segments, time_segments, regions)\nprint(f\"region_de shape: {region_de.shape}, region_psd shape: {region_psd.shape}, region_time shape: {region_time.shape}\")\n\n# GCN层定义\nclass GCNLayer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n        self.bn = nn.BatchNorm1d(out_features)\n        nn.init.xavier_normal_(self.weight)\n    \n    def forward(self, x, adj):\n        batch_size = x.size(0)\n        support = torch.bmm(x, self.weight.unsqueeze(0).expand(batch_size, -1, -1))\n        adj_batch = adj.unsqueeze(0).expand(batch_size, -1, -1)  # 扩展为 [batch_size, num_nodes, num_nodes]\n        output = torch.bmm(adj_batch, support)\n        output = output.transpose(1, 2).contiguous()\n        output = self.bn(output).transpose(1, 2)\n        return output\n\n# Transformer 层\nclass TransformerLayer(nn.Module):\n    def __init__(self, d_model, nhead=4):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(d_model, nhead)\n        self.norm = nn.LayerNorm(d_model)\n        self.ff = nn.Sequential(\n            nn.Linear(d_model, d_model * 2),\n            nn.ReLU(),\n            nn.Linear(d_model * 2, d_model)\n        )\n    \n    def forward(self, x):\n        x = x.transpose(0, 1)  # [nodes, batch, features]\n        attn_output, _ = self.attn(x, x, x)\n        x = self.norm(x + attn_output)\n        ff_output = self.ff(x)\n        x = self.norm(x + ff_output)\n        return x.transpose(0, 1)\n\n# 自适应特征选择模块\nclass FeatureSelector(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.weights = nn.Parameter(torch.ones(in_features))\n        self.fc = nn.Linear(in_features, out_features)\n    \n    def forward(self, x):\n        weights = F.softmax(self.weights, dim=0)\n        x = x * weights.view(1, 1, -1)  # 自适应加权\n        return F.relu(self.fc(x))\n\n\nclass GraphTimeNet(nn.Module):\n    def __init__(self, in_features=12, hidden_features=64, num_classes=2):\n        super().__init__()\n        self.selector = FeatureSelector(in_features, in_features // 2)  # 12 -> 6\n        self.gcn1 = GCNLayer(in_features // 2, hidden_features)\n        self.gcn2 = GCNLayer(hidden_features, hidden_features // 2)\n        self.transformer = TransformerLayer(hidden_features // 2)\n        self.fc_valence = nn.Linear((hidden_features // 2) * num_nodes, num_classes)\n        self.fc_arousal = nn.Linear((hidden_features // 2) * num_nodes, num_classes)\n        self.dropout = nn.Dropout(0.3)\n    \n    def forward(self, x_de, x_psd, x_time, adj):\n        x = torch.cat([x_de, x_psd, x_time], dim=-1)  # [batch_size, num_nodes, 12]\n        x = self.selector(x)  # [batch_size, num_nodes, 6]\n        x = F.relu(self.gcn1(x, adj))\n        x = F.relu(self.gcn2(x, adj))\n        x = self.transformer(x)\n        x = x.contiguous().view(x.size(0), -1)\n        x = self.dropout(x)\n        valence_out = self.fc_valence(x)\n        arousal_out = self.fc_arousal(x)\n        return valence_out, arousal_out\n\n\nX_de = torch.tensor(region_de, dtype=torch.float32).to(device)\nX_psd = torch.tensor(region_psd, dtype=torch.float32).to(device)\nX_time = torch.tensor(region_time, dtype=torch.float32).to(device)\ny_valence = torch.tensor(valence_labels, dtype=torch.long).to(device)\ny_arousal = torch.tensor(arousal_labels, dtype=torch.long).to(device)\nprint(f\"X_de shape: {X_de.shape}, X_psd shape: {X_psd.shape}, X_time shape: {X_time.shape}\")\n\n# 构建固定邻接矩阵\nadj = torch.ones(num_nodes, num_nodes, device=device) - torch.eye(num_nodes, device=device)\nadj = adj / (adj.sum(dim=1, keepdim=True) + 1e-8)\n\n\ndef train_and_evaluate(X_de, X_psd, X_time, y_valence, y_arousal):\n    X_de_train, X_de_test, X_psd_train, X_psd_test, X_time_train, X_time_test, \\\n    y_val_train, y_val_test, y_aro_train, y_aro_test = train_test_split(\n        X_de, X_psd, X_time, y_valence, y_arousal, test_size=0.2, random_state=42, shuffle=True\n    )\n    train_dataset = torch.utils.data.TensorDataset(X_de_train, X_psd_train, X_time_train, y_val_train, y_aro_train)\n    test_dataset = torch.utils.data.TensorDataset(X_de_test, X_psd_test, X_time_test, y_val_test, y_aro_test)\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n    \n    model = GraphTimeNet().to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n    criterion = nn.CrossEntropyLoss()\n    \n    train_losses = []\n    val_accuracies = []\n    aro_accuracies = []\n    best_val_acc = 0\n    best_aro_acc = 0\n    \n    for epoch in range(200):\n        model.train()\n        train_loss = 0\n        for x_de_batch, x_psd_batch, x_time_batch, y_val_batch, y_aro_batch in train_loader:\n            optimizer.zero_grad()\n            val_out, aro_out = model(x_de_batch.transpose(1, 2), x_psd_batch.transpose(1, 2), x_time_batch.transpose(1, 2), adj)\n            loss_val = criterion(val_out, y_val_batch)\n            loss_aro = criterion(aro_out, y_aro_batch)\n            loss = 0.5 * (loss_val + loss_aro)  # 多任务损失平衡\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        train_losses.append(train_loss / len(train_loader))\n        \n        model.eval()\n        val_correct = 0\n        aro_correct = 0\n        total = 0\n        with torch.no_grad():\n            for x_de_batch, x_psd_batch, x_time_batch, y_val_batch, y_aro_batch in test_loader:\n                val_out, aro_out = model(x_de_batch.transpose(1, 2), x_psd_batch.transpose(1, 2), x_time_batch.transpose(1, 2), adj)\n                val_pred = val_out.argmax(dim=1)\n                aro_pred = aro_out.argmax(dim=1)\n                val_correct += (val_pred == y_val_batch).sum().item()\n                aro_correct += (aro_pred == y_aro_batch).sum().item()\n                total += y_val_batch.size(0)\n        val_acc = val_correct / total\n        aro_acc = aro_correct / total\n        val_accuracies.append(val_acc)\n        aro_accuracies.append(aro_acc)\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n        if aro_acc > best_aro_acc:\n            best_aro_acc = aro_acc\n        \n        print(f\"Epoch {epoch+1}: Train Loss: {train_losses[-1]:.4f}, Val Acc: {val_acc:.4f}, Aro Acc: {aro_acc:.4f}\")\n    \n    print(f\"Best Valence Test Accuracy: {best_val_acc:.4f}\")\n    print(f\"Best Arousal Test Accuracy: {best_aro_acc:.4f}\")\n    return best_val_acc, best_aro_acc\n\nprint(\"Training Multi-Task Model:\")\nval_acc, aro_acc = train_and_evaluate(X_de, X_psd, X_time, y_valence, y_arousal)\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:16:38.719025Z","iopub.execute_input":"2025-04-05T07:16:38.719349Z","iopub.status.idle":"2025-04-05T07:30:33.344737Z","shell.execute_reply.started":"2025-04-05T07:16:38.719323Z","shell.execute_reply":"2025-04-05T07:30:33.343939Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nregion_de shape: (26880, 4, 5), region_psd shape: (26880, 4, 5), region_time shape: (26880, 4, 5)\nX_de shape: torch.Size([26880, 4, 5]), X_psd shape: torch.Size([26880, 4, 5]), X_time shape: torch.Size([26880, 4, 5])\nTraining Multi-Task Model:\nEpoch 1: Train Loss: 0.6934, Val Acc: 0.5539, Aro Acc: 0.5971\nEpoch 2: Train Loss: 0.6769, Val Acc: 0.5653, Aro Acc: 0.5476\nEpoch 3: Train Loss: 0.6718, Val Acc: 0.5618, Aro Acc: 0.6090\nEpoch 4: Train Loss: 0.6685, Val Acc: 0.5763, Aro Acc: 0.5999\nEpoch 5: Train Loss: 0.6666, Val Acc: 0.5634, Aro Acc: 0.6287\nEpoch 6: Train Loss: 0.6629, Val Acc: 0.5809, Aro Acc: 0.6153\nEpoch 7: Train Loss: 0.6600, Val Acc: 0.5858, Aro Acc: 0.6137\nEpoch 8: Train Loss: 0.6588, Val Acc: 0.5761, Aro Acc: 0.5921\nEpoch 9: Train Loss: 0.6559, Val Acc: 0.6027, Aro Acc: 0.6334\nEpoch 10: Train Loss: 0.6531, Val Acc: 0.6150, Aro Acc: 0.6168\nEpoch 11: Train Loss: 0.6470, Val Acc: 0.5547, Aro Acc: 0.6254\nEpoch 12: Train Loss: 0.6441, Val Acc: 0.6071, Aro Acc: 0.6408\nEpoch 13: Train Loss: 0.6419, Val Acc: 0.5895, Aro Acc: 0.6129\nEpoch 14: Train Loss: 0.6358, Val Acc: 0.6133, Aro Acc: 0.6672\nEpoch 15: Train Loss: 0.6282, Val Acc: 0.6073, Aro Acc: 0.6561\nEpoch 16: Train Loss: 0.6256, Val Acc: 0.6270, Aro Acc: 0.6730\nEpoch 17: Train Loss: 0.6217, Val Acc: 0.6081, Aro Acc: 0.6496\nEpoch 18: Train Loss: 0.6122, Val Acc: 0.6291, Aro Acc: 0.6641\nEpoch 19: Train Loss: 0.6099, Val Acc: 0.5647, Aro Acc: 0.6181\nEpoch 20: Train Loss: 0.6047, Val Acc: 0.6382, Aro Acc: 0.6789\nEpoch 21: Train Loss: 0.5965, Val Acc: 0.6189, Aro Acc: 0.6680\nEpoch 22: Train Loss: 0.5920, Val Acc: 0.6319, Aro Acc: 0.6652\nEpoch 23: Train Loss: 0.5848, Val Acc: 0.6434, Aro Acc: 0.6503\nEpoch 24: Train Loss: 0.5775, Val Acc: 0.6646, Aro Acc: 0.7249\nEpoch 25: Train Loss: 0.5717, Val Acc: 0.5977, Aro Acc: 0.6440\nEpoch 26: Train Loss: 0.5678, Val Acc: 0.6620, Aro Acc: 0.7024\nEpoch 27: Train Loss: 0.5646, Val Acc: 0.6451, Aro Acc: 0.6297\nEpoch 28: Train Loss: 0.5609, Val Acc: 0.6436, Aro Acc: 0.6728\nEpoch 29: Train Loss: 0.5502, Val Acc: 0.6908, Aro Acc: 0.6830\nEpoch 30: Train Loss: 0.5435, Val Acc: 0.6609, Aro Acc: 0.6538\nEpoch 31: Train Loss: 0.5388, Val Acc: 0.6695, Aro Acc: 0.7141\nEpoch 32: Train Loss: 0.5326, Val Acc: 0.6017, Aro Acc: 0.6401\nEpoch 33: Train Loss: 0.5250, Val Acc: 0.7024, Aro Acc: 0.7225\nEpoch 34: Train Loss: 0.5202, Val Acc: 0.7228, Aro Acc: 0.7643\nEpoch 35: Train Loss: 0.5114, Val Acc: 0.5763, Aro Acc: 0.6512\nEpoch 36: Train Loss: 0.5078, Val Acc: 0.5900, Aro Acc: 0.6203\nEpoch 37: Train Loss: 0.4991, Val Acc: 0.7716, Aro Acc: 0.7913\nEpoch 38: Train Loss: 0.4948, Val Acc: 0.7186, Aro Acc: 0.7450\nEpoch 39: Train Loss: 0.4944, Val Acc: 0.6412, Aro Acc: 0.6920\nEpoch 40: Train Loss: 0.4804, Val Acc: 0.6881, Aro Acc: 0.7254\nEpoch 41: Train Loss: 0.4816, Val Acc: 0.6440, Aro Acc: 0.6430\nEpoch 42: Train Loss: 0.4781, Val Acc: 0.7770, Aro Acc: 0.7736\nEpoch 43: Train Loss: 0.4724, Val Acc: 0.7238, Aro Acc: 0.7586\nEpoch 44: Train Loss: 0.4681, Val Acc: 0.7626, Aro Acc: 0.7812\nEpoch 45: Train Loss: 0.4610, Val Acc: 0.7080, Aro Acc: 0.7489\nEpoch 46: Train Loss: 0.4636, Val Acc: 0.7839, Aro Acc: 0.8227\nEpoch 47: Train Loss: 0.4541, Val Acc: 0.7866, Aro Acc: 0.8025\nEpoch 48: Train Loss: 0.4524, Val Acc: 0.7926, Aro Acc: 0.8257\nEpoch 49: Train Loss: 0.4509, Val Acc: 0.6306, Aro Acc: 0.5930\nEpoch 50: Train Loss: 0.4444, Val Acc: 0.7677, Aro Acc: 0.8108\nEpoch 51: Train Loss: 0.4411, Val Acc: 0.7048, Aro Acc: 0.7474\nEpoch 52: Train Loss: 0.4402, Val Acc: 0.7141, Aro Acc: 0.7721\nEpoch 53: Train Loss: 0.4346, Val Acc: 0.7805, Aro Acc: 0.7969\nEpoch 54: Train Loss: 0.4278, Val Acc: 0.6416, Aro Acc: 0.6936\nEpoch 55: Train Loss: 0.4336, Val Acc: 0.5549, Aro Acc: 0.5011\nEpoch 56: Train Loss: 0.4229, Val Acc: 0.8240, Aro Acc: 0.8495\nEpoch 57: Train Loss: 0.4245, Val Acc: 0.8415, Aro Acc: 0.8635\nEpoch 58: Train Loss: 0.4171, Val Acc: 0.7829, Aro Acc: 0.7993\nEpoch 59: Train Loss: 0.4079, Val Acc: 0.8251, Aro Acc: 0.8330\nEpoch 60: Train Loss: 0.4088, Val Acc: 0.6966, Aro Acc: 0.7476\nEpoch 61: Train Loss: 0.4035, Val Acc: 0.8419, Aro Acc: 0.8464\nEpoch 62: Train Loss: 0.4046, Val Acc: 0.8119, Aro Acc: 0.8352\nEpoch 63: Train Loss: 0.4017, Val Acc: 0.8216, Aro Acc: 0.8268\nEpoch 64: Train Loss: 0.3987, Val Acc: 0.8300, Aro Acc: 0.8529\nEpoch 65: Train Loss: 0.3931, Val Acc: 0.7811, Aro Acc: 0.8071\nEpoch 66: Train Loss: 0.3917, Val Acc: 0.7174, Aro Acc: 0.7597\nEpoch 67: Train Loss: 0.3899, Val Acc: 0.8222, Aro Acc: 0.8356\nEpoch 68: Train Loss: 0.3885, Val Acc: 0.8077, Aro Acc: 0.8348\nEpoch 69: Train Loss: 0.3796, Val Acc: 0.8326, Aro Acc: 0.8346\nEpoch 70: Train Loss: 0.3806, Val Acc: 0.7660, Aro Acc: 0.8065\nEpoch 71: Train Loss: 0.3794, Val Acc: 0.8231, Aro Acc: 0.8192\nEpoch 72: Train Loss: 0.3758, Val Acc: 0.6040, Aro Acc: 0.5631\nEpoch 73: Train Loss: 0.3788, Val Acc: 0.8681, Aro Acc: 0.8886\nEpoch 74: Train Loss: 0.3716, Val Acc: 0.8400, Aro Acc: 0.8527\nEpoch 75: Train Loss: 0.3627, Val Acc: 0.8019, Aro Acc: 0.8428\nEpoch 76: Train Loss: 0.3679, Val Acc: 0.8367, Aro Acc: 0.8484\nEpoch 77: Train Loss: 0.3646, Val Acc: 0.7485, Aro Acc: 0.7840\nEpoch 78: Train Loss: 0.3633, Val Acc: 0.8082, Aro Acc: 0.8477\nEpoch 79: Train Loss: 0.3624, Val Acc: 0.8754, Aro Acc: 0.8847\nEpoch 80: Train Loss: 0.3644, Val Acc: 0.6278, Aro Acc: 0.6244\nEpoch 81: Train Loss: 0.3556, Val Acc: 0.8311, Aro Acc: 0.8650\nEpoch 82: Train Loss: 0.3526, Val Acc: 0.8393, Aro Acc: 0.8499\nEpoch 83: Train Loss: 0.3508, Val Acc: 0.8021, Aro Acc: 0.8004\nEpoch 84: Train Loss: 0.3530, Val Acc: 0.8047, Aro Acc: 0.7904\nEpoch 85: Train Loss: 0.3473, Val Acc: 0.7645, Aro Acc: 0.8043\nEpoch 86: Train Loss: 0.3456, Val Acc: 0.8863, Aro Acc: 0.8769\nEpoch 87: Train Loss: 0.3499, Val Acc: 0.8170, Aro Acc: 0.8458\nEpoch 88: Train Loss: 0.3487, Val Acc: 0.8132, Aro Acc: 0.8201\nEpoch 89: Train Loss: 0.3420, Val Acc: 0.8663, Aro Acc: 0.8631\nEpoch 90: Train Loss: 0.3396, Val Acc: 0.8873, Aro Acc: 0.9016\nEpoch 91: Train Loss: 0.3294, Val Acc: 0.7241, Aro Acc: 0.7500\nEpoch 92: Train Loss: 0.3305, Val Acc: 0.8566, Aro Acc: 0.8754\nEpoch 93: Train Loss: 0.3339, Val Acc: 0.9176, Aro Acc: 0.9094\nEpoch 94: Train Loss: 0.3258, Val Acc: 0.8393, Aro Acc: 0.8495\nEpoch 95: Train Loss: 0.3258, Val Acc: 0.7219, Aro Acc: 0.7790\nEpoch 96: Train Loss: 0.3294, Val Acc: 0.8823, Aro Acc: 0.8938\nEpoch 97: Train Loss: 0.3212, Val Acc: 0.6382, Aro Acc: 0.6527\nEpoch 98: Train Loss: 0.3279, Val Acc: 0.8203, Aro Acc: 0.8188\nEpoch 99: Train Loss: 0.3231, Val Acc: 0.8132, Aro Acc: 0.8385\nEpoch 100: Train Loss: 0.3224, Val Acc: 0.7666, Aro Acc: 0.7591\nEpoch 101: Train Loss: 0.3193, Val Acc: 0.8596, Aro Acc: 0.8769\nEpoch 102: Train Loss: 0.3120, Val Acc: 0.8545, Aro Acc: 0.8676\nEpoch 103: Train Loss: 0.3108, Val Acc: 0.9074, Aro Acc: 0.8899\nEpoch 104: Train Loss: 0.3046, Val Acc: 0.8356, Aro Acc: 0.8237\nEpoch 105: Train Loss: 0.3072, Val Acc: 0.8856, Aro Acc: 0.8865\nEpoch 106: Train Loss: 0.3051, Val Acc: 0.8659, Aro Acc: 0.8973\nEpoch 107: Train Loss: 0.3020, Val Acc: 0.8843, Aro Acc: 0.8841\nEpoch 108: Train Loss: 0.3021, Val Acc: 0.8436, Aro Acc: 0.8330\nEpoch 109: Train Loss: 0.3041, Val Acc: 0.8689, Aro Acc: 0.8830\nEpoch 110: Train Loss: 0.2988, Val Acc: 0.8687, Aro Acc: 0.8860\nEpoch 111: Train Loss: 0.2937, Val Acc: 0.8028, Aro Acc: 0.8322\nEpoch 112: Train Loss: 0.2928, Val Acc: 0.8761, Aro Acc: 0.8921\nEpoch 113: Train Loss: 0.2968, Val Acc: 0.8398, Aro Acc: 0.8330\nEpoch 114: Train Loss: 0.2980, Val Acc: 0.9096, Aro Acc: 0.9126\nEpoch 115: Train Loss: 0.2860, Val Acc: 0.8056, Aro Acc: 0.8158\nEpoch 116: Train Loss: 0.2884, Val Acc: 0.8093, Aro Acc: 0.8415\nEpoch 117: Train Loss: 0.2899, Val Acc: 0.7766, Aro Acc: 0.8118\nEpoch 118: Train Loss: 0.2817, Val Acc: 0.8908, Aro Acc: 0.9012\nEpoch 119: Train Loss: 0.2827, Val Acc: 0.7301, Aro Acc: 0.7504\nEpoch 120: Train Loss: 0.2818, Val Acc: 0.9262, Aro Acc: 0.9312\nEpoch 121: Train Loss: 0.2832, Val Acc: 0.8186, Aro Acc: 0.8573\nEpoch 122: Train Loss: 0.2835, Val Acc: 0.9213, Aro Acc: 0.9291\nEpoch 123: Train Loss: 0.2736, Val Acc: 0.8663, Aro Acc: 0.8547\nEpoch 124: Train Loss: 0.2779, Val Acc: 0.6014, Aro Acc: 0.6347\nEpoch 125: Train Loss: 0.2826, Val Acc: 0.8389, Aro Acc: 0.8756\nEpoch 126: Train Loss: 0.2763, Val Acc: 0.9055, Aro Acc: 0.9180\nEpoch 127: Train Loss: 0.2802, Val Acc: 0.9009, Aro Acc: 0.9122\nEpoch 128: Train Loss: 0.2765, Val Acc: 0.9042, Aro Acc: 0.9062\nEpoch 129: Train Loss: 0.2731, Val Acc: 0.8698, Aro Acc: 0.8878\nEpoch 130: Train Loss: 0.2791, Val Acc: 0.9135, Aro Acc: 0.9206\nEpoch 131: Train Loss: 0.2736, Val Acc: 0.8237, Aro Acc: 0.8214\nEpoch 132: Train Loss: 0.2677, Val Acc: 0.7755, Aro Acc: 0.7939\nEpoch 133: Train Loss: 0.2659, Val Acc: 0.9256, Aro Acc: 0.9239\nEpoch 134: Train Loss: 0.2665, Val Acc: 0.8951, Aro Acc: 0.8966\nEpoch 135: Train Loss: 0.2619, Val Acc: 0.7076, Aro Acc: 0.7509\nEpoch 136: Train Loss: 0.2668, Val Acc: 0.8929, Aro Acc: 0.8917\nEpoch 137: Train Loss: 0.2690, Val Acc: 0.9407, Aro Acc: 0.9397\nEpoch 138: Train Loss: 0.2658, Val Acc: 0.8756, Aro Acc: 0.8717\nEpoch 139: Train Loss: 0.2542, Val Acc: 0.8244, Aro Acc: 0.8119\nEpoch 140: Train Loss: 0.2680, Val Acc: 0.9453, Aro Acc: 0.9325\nEpoch 141: Train Loss: 0.2581, Val Acc: 0.8638, Aro Acc: 0.8763\nEpoch 142: Train Loss: 0.2602, Val Acc: 0.8921, Aro Acc: 0.9036\nEpoch 143: Train Loss: 0.2590, Val Acc: 0.8281, Aro Acc: 0.8421\nEpoch 144: Train Loss: 0.2583, Val Acc: 0.6882, Aro Acc: 0.6879\nEpoch 145: Train Loss: 0.2547, Val Acc: 0.7057, Aro Acc: 0.7307\nEpoch 146: Train Loss: 0.2542, Val Acc: 0.8454, Aro Acc: 0.8583\nEpoch 147: Train Loss: 0.2496, Val Acc: 0.9226, Aro Acc: 0.9280\nEpoch 148: Train Loss: 0.2482, Val Acc: 0.6823, Aro Acc: 0.6691\nEpoch 149: Train Loss: 0.2575, Val Acc: 0.8746, Aro Acc: 0.8996\nEpoch 150: Train Loss: 0.2469, Val Acc: 0.8302, Aro Acc: 0.8512\nEpoch 151: Train Loss: 0.2476, Val Acc: 0.9241, Aro Acc: 0.9178\nEpoch 152: Train Loss: 0.2464, Val Acc: 0.8112, Aro Acc: 0.8218\nEpoch 153: Train Loss: 0.2486, Val Acc: 0.8737, Aro Acc: 0.8856\nEpoch 154: Train Loss: 0.2450, Val Acc: 0.9055, Aro Acc: 0.9129\nEpoch 155: Train Loss: 0.2445, Val Acc: 0.8318, Aro Acc: 0.8281\nEpoch 156: Train Loss: 0.2472, Val Acc: 0.9159, Aro Acc: 0.9133\nEpoch 157: Train Loss: 0.2435, Val Acc: 0.8064, Aro Acc: 0.8371\nEpoch 158: Train Loss: 0.2391, Val Acc: 0.7407, Aro Acc: 0.7372\nEpoch 159: Train Loss: 0.2437, Val Acc: 0.8069, Aro Acc: 0.8054\nEpoch 160: Train Loss: 0.2391, Val Acc: 0.8692, Aro Acc: 0.9102\nEpoch 161: Train Loss: 0.2348, Val Acc: 0.9025, Aro Acc: 0.9094\nEpoch 162: Train Loss: 0.2374, Val Acc: 0.8430, Aro Acc: 0.8707\nEpoch 163: Train Loss: 0.2455, Val Acc: 0.7848, Aro Acc: 0.8101\nEpoch 164: Train Loss: 0.2394, Val Acc: 0.7240, Aro Acc: 0.7716\nEpoch 165: Train Loss: 0.2371, Val Acc: 0.8679, Aro Acc: 0.8808\nEpoch 166: Train Loss: 0.2346, Val Acc: 0.8077, Aro Acc: 0.8402\nEpoch 167: Train Loss: 0.2299, Val Acc: 0.8661, Aro Acc: 0.9007\nEpoch 168: Train Loss: 0.2348, Val Acc: 0.9219, Aro Acc: 0.9418\nEpoch 169: Train Loss: 0.2359, Val Acc: 0.9330, Aro Acc: 0.9407\nEpoch 170: Train Loss: 0.2304, Val Acc: 0.6628, Aro Acc: 0.6628\nEpoch 171: Train Loss: 0.2296, Val Acc: 0.9343, Aro Acc: 0.9362\nEpoch 172: Train Loss: 0.2285, Val Acc: 0.7985, Aro Acc: 0.8438\nEpoch 173: Train Loss: 0.2333, Val Acc: 0.8873, Aro Acc: 0.9033\nEpoch 174: Train Loss: 0.2315, Val Acc: 0.8798, Aro Acc: 0.9046\nEpoch 175: Train Loss: 0.2332, Val Acc: 0.9600, Aro Acc: 0.9715\nEpoch 176: Train Loss: 0.2237, Val Acc: 0.9090, Aro Acc: 0.9209\nEpoch 177: Train Loss: 0.2302, Val Acc: 0.8116, Aro Acc: 0.8339\nEpoch 178: Train Loss: 0.2290, Val Acc: 0.8782, Aro Acc: 0.8860\nEpoch 179: Train Loss: 0.2218, Val Acc: 0.6799, Aro Acc: 0.7130\nEpoch 180: Train Loss: 0.2278, Val Acc: 0.8374, Aro Acc: 0.8475\nEpoch 181: Train Loss: 0.2245, Val Acc: 0.8045, Aro Acc: 0.8404\nEpoch 182: Train Loss: 0.2306, Val Acc: 0.8969, Aro Acc: 0.9077\nEpoch 183: Train Loss: 0.2191, Val Acc: 0.9312, Aro Acc: 0.9353\nEpoch 184: Train Loss: 0.2234, Val Acc: 0.8888, Aro Acc: 0.9040\nEpoch 185: Train Loss: 0.2217, Val Acc: 0.8927, Aro Acc: 0.9087\nEpoch 186: Train Loss: 0.2203, Val Acc: 0.9609, Aro Acc: 0.9587\nEpoch 187: Train Loss: 0.2279, Val Acc: 0.8356, Aro Acc: 0.8700\nEpoch 188: Train Loss: 0.2147, Val Acc: 0.8964, Aro Acc: 0.9057\nEpoch 189: Train Loss: 0.2201, Val Acc: 0.8367, Aro Acc: 0.8464\nEpoch 190: Train Loss: 0.2112, Val Acc: 0.5746, Aro Acc: 0.6228\nEpoch 191: Train Loss: 0.2143, Val Acc: 0.7679, Aro Acc: 0.8172\nEpoch 192: Train Loss: 0.2148, Val Acc: 0.9230, Aro Acc: 0.9327\nEpoch 193: Train Loss: 0.2182, Val Acc: 0.7662, Aro Acc: 0.8207\nEpoch 194: Train Loss: 0.2151, Val Acc: 0.8021, Aro Acc: 0.8385\nEpoch 195: Train Loss: 0.2208, Val Acc: 0.8651, Aro Acc: 0.8839\nEpoch 196: Train Loss: 0.2169, Val Acc: 0.7184, Aro Acc: 0.7102\nEpoch 197: Train Loss: 0.2144, Val Acc: 0.8318, Aro Acc: 0.8627\nEpoch 198: Train Loss: 0.2128, Val Acc: 0.8225, Aro Acc: 0.8136\nEpoch 199: Train Loss: 0.2109, Val Acc: 0.9643, Aro Acc: 0.9496\nEpoch 200: Train Loss: 0.2148, Val Acc: 0.9148, Aro Acc: 0.9198\nBest Valence Test Accuracy: 0.9643\nBest Arousal Test Accuracy: 0.9715\n","output_type":"stream"}],"execution_count":1}]}